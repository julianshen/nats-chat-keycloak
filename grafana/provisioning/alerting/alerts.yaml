apiVersion: 1

contactPoints:
  - orgId: 1
    name: grafana-default-email
    receivers:
      - uid: grafana-default-email
        type: email
        settings:
          addresses: admin@localhost
        disableResolveMessage: true

policies:
  - orgId: 1
    receiver: grafana-default-email
    group_by:
      - grafana_folder
      - alertname

groups:
  - orgId: 1
    name: service-health
    folder: Alerts
    interval: 30s
    rules:
      - uid: service-down-auth
        title: "ServiceDown: auth-service"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: increase(traces_spanmetrics_calls_total{service_name="auth-service"}[5m])
              instant: true
              refId: A
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: A
              conditions:
                - evaluator:
                    type: lt
                    params:
                      - 0.001
              refId: C
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "auth-service is down (no spans received in 5m)"

      - uid: service-down-fanout
        title: "ServiceDown: fanout-service"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: increase(traces_spanmetrics_calls_total{service_name="fanout-service"}[5m])
              instant: true
              refId: A
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: A
              conditions:
                - evaluator:
                    type: lt
                    params:
                      - 0.001
              refId: C
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "fanout-service is down (no spans received in 5m)"

      - uid: service-down-room
        title: "ServiceDown: room-service"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: increase(traces_spanmetrics_calls_total{service_name="room-service"}[5m])
              instant: true
              refId: A
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: A
              conditions:
                - evaluator:
                    type: lt
                    params:
                      - 0.001
              refId: C
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "room-service is down (no spans received in 5m)"

      - uid: service-down-presence
        title: "ServiceDown: presence-service"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: increase(traces_spanmetrics_calls_total{service_name="presence-service"}[5m])
              instant: true
              refId: A
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: A
              conditions:
                - evaluator:
                    type: lt
                    params:
                      - 0.001
              refId: C
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "presence-service is down (no spans received in 5m)"

      - uid: service-down-persist
        title: "ServiceDown: persist-worker"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: increase(traces_spanmetrics_calls_total{service_name="persist-worker"}[5m])
              instant: true
              refId: A
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: A
              conditions:
                - evaluator:
                    type: lt
                    params:
                      - 0.001
              refId: C
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "persist-worker is down (no spans received in 5m)"

      - uid: service-down-history
        title: "ServiceDown: history-service"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: increase(traces_spanmetrics_calls_total{service_name="history-service"}[5m])
              instant: true
              refId: A
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: A
              conditions:
                - evaluator:
                    type: lt
                    params:
                      - 0.001
              refId: C
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "history-service is down (no spans received in 5m)"

  - orgId: 1
    name: error-rates
    folder: Alerts
    interval: 30s
    rules:
      - uid: high-error-rate
        title: "High Error Rate"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: |
                (
                  sum by (service_name) (rate(traces_spanmetrics_calls_total{status_code="STATUS_CODE_ERROR"}[5m]))
                  /
                  sum by (service_name) (rate(traces_spanmetrics_calls_total[5m]))
                )
              instant: true
              refId: A
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: A
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 0.05
              refId: C
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Error rate exceeds 5% for a service"

      - uid: persist-worker-errors
        title: "Persist Worker Errors"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: increase(messages_persist_errors_total[5m])
              instant: true
              refId: A
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: A
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 5
              refId: C
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Persist worker has more than 5 errors in 5 minutes"

      - uid: auth-rejection-spike
        title: "Auth Rejection Spike"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: |
                (
                  sum(rate(auth_requests_total{result="rejected"}[5m]))
                  /
                  sum(rate(auth_requests_total[5m]))
                )
              instant: true
              refId: A
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: A
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 0.20
              refId: C
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Auth rejection rate exceeds 20%"

  - orgId: 1
    name: latency
    folder: Alerts
    interval: 30s
    rules:
      - uid: slow-requests
        title: "Slow Requests"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: histogram_quantile(0.95, sum by (le, service_name) (rate(traces_spanmetrics_duration_seconds_bucket[5m])))
              instant: true
              refId: A
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: A
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 2
              refId: C
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "P95 request latency exceeds 2 seconds"

      - uid: high-fanout-latency
        title: "High Fanout Latency"
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: histogram_quantile(0.95, sum by (le) (rate(fanout_duration_seconds_bucket[5m])))
              instant: true
              refId: A
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              type: threshold
              expression: A
              conditions:
                - evaluator:
                    type: gt
                    params:
                      - 0.5
              refId: C
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Fanout P95 latency exceeds 500ms"
